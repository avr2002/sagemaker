#!/bin/bash

set -euox pipefail

# Source the .env file if it exists
[ -f .env ] && source .env || echo ".env file not found!" && exit 1

export AWS_PROFILE=sandbox
export AWS_REGION=us-east-1
export AWS_DEFAULT_REGION=${AWS_REGION}

# aws account id/hash used to make a unique, but consistent bucket name
# export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --profile $AWS_PROFILE --query "Account" --output text)

# Cache AWS account ID for 10 hours using bash-cache
# AWS_ACCOUNT_ID=$(bc::cache 10h aws sts get-caller-identity --profile $AWS_PROFILE --query "Account" --output text)

# Cache AWS account ID for 10 hours using our custom function
AWS_ACCOUNT_ID=$(get-cached-aws-account-id 10)
AWS_ACCOUNT_ID_HASH=$(echo -n "${AWS_ACCOUNT_ID}" | sha256sum | cut -c5-8)
export S3_BUCKET_NAME="ml-school-bucket-${AWS_ACCOUNT_ID_HASH}" # ml-school-bucket-6721


THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

# Data file path
DATA_FILE_PATH="$THIS_DIR/data/penguins.csv" # Path to the data file on your local machine
FILE_NAME="$(basename "$DATA_FILE_PATH")"
S3_FILE_PATH="penguins/data/$FILE_NAME"      # Path to the data file in the S3 bucket


##########################
# --- Task Functions --- #
##########################

function run-pipeline () {
    if [[ "$1" == "--local" ]]; then
        export LOCAL_MODE=1
    fi
    uv run pipeline.py
}


function cdk-bootstrap () {
    npm install -g aws-cdk
    
    # Use cached account ID here too
    cdk bootstrap "aws://${AWS_ACCOUNT_ID}/${AWS_REGION}"
}

function cdk-deploy () {
    uv run -- cdk deploy --app 'python cdk_app.py' '*' --profile $AWS_PROFILE --region $AWS_REGION
}

function cdk-destroy () {
    uv run -- cdk destroy --app 'python cdk_app.py' '*' --profile $AWS_PROFILE --region $AWS_REGION
}


# Install Python dependencies into the currently activated venv
function install {
    # if uv is not installed, then update the commands as `python -m pip install --upgrade pip`
	uv run pip install --upgrade pip
	uv run pip install --editable "$THIS_DIR/[dev]"

	# python -m pip install --upgrade pip
	# python -m pip install --editable "$THIS_DIR/[dev]"

    # If using uv and you are getting pyyaml compilation error during installation then follow the below thread:
    # https://github.com/astral-sh/uv/issues/1455
    # This error doesn't occur when using the normal python command.
    # The error happend with python version 3.10.x [NOT SURE]
}


function run-docker {
    # Remove old AWS credentials
    remove-old-aws-credentials

    # Append new AWS credentials to .env
    aws configure export-credentials --profile "$AWS_PROFILE" --format env > .env-aws

    # Build the Docker image and push it to ECR
    # docker-build-image-and-push-to-ecr

    # Run the Docker container
    docker compose up --remove-orphans --build
}


function docker-build-image-and-push-to-ecr {
    set -ex

    local repository_name=$1
    local dockerfile_path=$2

    # Check if docker is running
    if ! docker info > /dev/null 2>&1; then
        echo "Error: Docker is not running."
        return 1
    fi

    # Get the AWS account ID
    # AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
    ECR_URI="$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$repository_name:latest"

    # Build the Docker image
    if [ -d "$THIS_DIR/$dockerfile_path" ] || [ -f "$THIS_DIR/$dockerfile_path" ]; then
        docker build --tag $repository_name $THIS_DIR/$dockerfile_path
    else
        echo "Error: $dockerfile_path is not a directory or a file."
        return 1
    fi

    # Authenticate Docker to the ECR
    aws ecr get-login-password --region $AWS_REGION \
        | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

    # Ensure that the repository exists
    aws ecr describe-repositories --repository-names "$repository_name" --region ${AWS_REGION} > /dev/null 2>&1 || \
    aws ecr create-repository --repository-name "${repository_name}" --region ${AWS_REGION} > /dev/null

    # Tag and push the Docker image to the ECR
    docker tag $repository_name:latest $ECR_URI
    docker push $ECR_URI
}


# Function to remove old AWS credentials from the .env file
function remove-old-aws-credentials {
    # Delete any lines that export AWS credentials
    sed -i '' '/^export AWS_ACCESS_KEY_ID/d' .env
    sed -i '' '/^export AWS_SECRET_ACCESS_KEY/d' .env
    sed -i '' '/^export AWS_SESSION_TOKEN/d' .env
    sed -i '' '/^export AWS_CREDENTIAL_EXPIRATION/d' .env

    # Ensure there is a new line at the end of the .env file if it's missing, so that the new credentials are on a new line
    if [ "$(tail -c 1 .env)" != "" ]; then
        echo "" >> .env
    fi
}


# run linting, formatting, and other static code quality tools
function lint {
	uvx --from pre-commit pre-commit run --all-files
}

# same as `lint` but with any special considerations for CI
function lint:ci {
	# We skip no-commit-to-branch since that blocks commits to `main`.
	# All merged PRs are commits to `main` so this must be disabled.
	SKIP=no-commit-to-branch uvx --from pre-commit pre-commit run --all-files
}


# AWS Account ID Caching Strategy:
# To avoid making expensive AWS STS API calls on every script execution,
# we cache the AWS account ID in a temporary file for 24 hours.
# This approach:
# - Stores cache in /tmp/.aws_account_id_cache_${AWS_PROFILE}
# - Uses file modification time to check expiration (24 hours)
# - Maintains separate caches per AWS profile
# - Automatically refreshes when expired
# - Can be manually cleared with the 'clean' function

#####
# Alternative: Consider using 'bash-cache' utility for more sophisticated caching
# GitHub Link: https://github.com/dimo414/bash-cache/blob/master/bash-cache.sh
# Install bash-cache
# source path/to/bash-cache/bc.bash
# I downloaded it at scripts/bash-cache.sh -- `source scripts/bash-cache.sh``

# Cache AWS account ID for 24 hours
# AWS_ACCOUNT_ID=$(bc::cache 24h aws sts get-caller-identity --profile $AWS_PROFILE --query "Account" --output text)


# Function to get cached AWS account ID
function get-cached-aws-account-id {
    local cache_duration_hours=${1:-10}  # Default to 10 hours if no argument provided
    local cache_file="/tmp/.aws_account_id_cache_${AWS_PROFILE}"
    local cache_duration=$((cache_duration_hours * 3600))  # Convert hours to seconds
    
    # Check if cache file exists and is not expired
    if [[ -f "$cache_file" ]]; then
        local file_age=$(( $(date +%s) - $(stat -f %m "$cache_file" 2>/dev/null || stat -c %Y "$cache_file" 2>/dev/null || echo 0) ))
        if [[ $file_age -lt $cache_duration ]]; then
            cat "$cache_file"
            return
        fi
    fi
    
    # Cache is expired or doesn't exist, fetch new account ID
    local account_id=$(aws sts get-caller-identity --profile $AWS_PROFILE --query "Account" --output text)
    echo "$account_id" > "$cache_file"
    echo "$account_id"
}
# Usage:
# get-cached-aws-account-id (uses default 10 hours)
# get-cached-aws-account-id 24 (caches for 24 hours)
# get-cached-aws-account-id 1 (caches for 1 hour)


# remove all files generated by tests, builds, or operating this codebase
function clean {
	rm -rf dist build coverage.xml test-reports
	find . \
	  -type d \
	  \( \
		-name "*cache*" \
		-o -name "*.dist-info" \
		-o -name "*.egg-info" \
		-o -name "*htmlcov" \
        -o -name "cdk.out" \
	  \) \
	  -not -path "*env*/*" \
	  -exec rm -r {} + || true

	find . \
	  -type f \
	  -name "*.pyc" \
	  -not -path "*env/*" \
	  -exec rm {} +
	
	# Clean AWS account ID cache
	rm -f /tmp/.aws_account_id_cache_*
}


# print all functions in this file
function help {
    echo "$0 <task> <args>"
    echo "Tasks:"
    compgen -A function | cat -n
}


TIMEFORMAT="Task completed in %3lR"
time ${@:-help}
